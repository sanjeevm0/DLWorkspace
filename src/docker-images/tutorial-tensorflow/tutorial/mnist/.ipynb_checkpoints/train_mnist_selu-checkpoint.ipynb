{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 28 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "    \n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "nb_train_samples = x_train.shape[0]\n",
    "nb_test_samples = x_test.shape[0]\n",
    "\n",
    "# define data preparation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255.,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2 )\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255.)\n",
    "\n",
    "# fit parameters from data\n",
    "train_datagen.fit(x_train)\n",
    "test_datagen.fit(x_test)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for x_batch, y_batch in train_datagen.flow(x_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(x_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 1.2167 - acc: 0.5926 - val_loss: 0.5559 - val_acc: 0.8214\n",
      "Epoch 2/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.8477 - acc: 0.7256 - val_loss: 0.3788 - val_acc: 0.8890\n",
      "Epoch 3/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.6818 - acc: 0.7799 - val_loss: 0.4523 - val_acc: 0.8546\n",
      "Epoch 4/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.5517 - acc: 0.8225 - val_loss: 0.3360 - val_acc: 0.8918\n",
      "Epoch 5/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.4547 - acc: 0.8572 - val_loss: 0.1876 - val_acc: 0.9423\n",
      "Epoch 6/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.4124 - acc: 0.8715 - val_loss: 0.1978 - val_acc: 0.9365\n",
      "Epoch 7/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.3735 - acc: 0.8827 - val_loss: 0.2252 - val_acc: 0.9281\n",
      "Epoch 8/200\n",
      "468/468 [==============================] - 18s 38ms/step - loss: 0.3459 - acc: 0.8941 - val_loss: 0.2550 - val_acc: 0.9211\n",
      "Epoch 9/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.3317 - acc: 0.8989 - val_loss: 0.1617 - val_acc: 0.9498\n",
      "Epoch 10/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.3183 - acc: 0.9024 - val_loss: 0.1145 - val_acc: 0.9658\n",
      "Epoch 11/200\n",
      "468/468 [==============================] - 25s 52ms/step - loss: 0.2956 - acc: 0.9087 - val_loss: 0.1105 - val_acc: 0.9666\n",
      "Epoch 12/200\n",
      "468/468 [==============================] - 21s 46ms/step - loss: 0.2846 - acc: 0.9121 - val_loss: 0.1494 - val_acc: 0.9542\n",
      "Epoch 13/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.2781 - acc: 0.9135 - val_loss: 0.1598 - val_acc: 0.9513\n",
      "Epoch 14/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.2724 - acc: 0.9156 - val_loss: 0.0990 - val_acc: 0.9700\n",
      "Epoch 15/200\n",
      "468/468 [==============================] - 23s 48ms/step - loss: 0.2621 - acc: 0.9197 - val_loss: 0.1002 - val_acc: 0.9694\n",
      "Epoch 16/200\n",
      "468/468 [==============================] - 25s 54ms/step - loss: 0.2612 - acc: 0.9203 - val_loss: 0.1680 - val_acc: 0.9498\n",
      "Epoch 17/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.2552 - acc: 0.9232 - val_loss: 0.1275 - val_acc: 0.9625\n",
      "Epoch 18/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.2517 - acc: 0.9237 - val_loss: 0.1743 - val_acc: 0.9475\n",
      "Epoch 19/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.2466 - acc: 0.9249 - val_loss: 0.0961 - val_acc: 0.9712\n",
      "Epoch 20/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.2400 - acc: 0.9272 - val_loss: 0.1170 - val_acc: 0.9654\n",
      "Epoch 21/200\n",
      "468/468 [==============================] - 20s 44ms/step - loss: 0.2425 - acc: 0.9267 - val_loss: 0.0905 - val_acc: 0.9717\n",
      "Epoch 22/200\n",
      "468/468 [==============================] - 24s 50ms/step - loss: 0.2384 - acc: 0.9276 - val_loss: 0.0859 - val_acc: 0.9769\n",
      "Epoch 23/200\n",
      "468/468 [==============================] - 20s 44ms/step - loss: 0.2308 - acc: 0.9304 - val_loss: 0.0921 - val_acc: 0.9712\n",
      "Epoch 24/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.2297 - acc: 0.9303 - val_loss: 0.0810 - val_acc: 0.9751\n",
      "Epoch 25/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.2246 - acc: 0.9325 - val_loss: 0.1070 - val_acc: 0.9707\n",
      "Epoch 26/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.2252 - acc: 0.9332 - val_loss: 0.1289 - val_acc: 0.9619\n",
      "Epoch 27/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.2252 - acc: 0.9309 - val_loss: 0.0912 - val_acc: 0.9723\n",
      "Epoch 28/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.2208 - acc: 0.9324 - val_loss: 0.0924 - val_acc: 0.9731\n",
      "Epoch 29/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.2201 - acc: 0.9343 - val_loss: 0.0728 - val_acc: 0.9778\n",
      "Epoch 30/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.2136 - acc: 0.9355 - val_loss: 0.0680 - val_acc: 0.9804\n",
      "Epoch 31/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.2165 - acc: 0.9340 - val_loss: 0.0765 - val_acc: 0.9777\n",
      "Epoch 32/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.2131 - acc: 0.9359 - val_loss: 0.0973 - val_acc: 0.9731\n",
      "Epoch 33/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.2134 - acc: 0.9361 - val_loss: 0.0752 - val_acc: 0.9796\n",
      "Epoch 34/200\n",
      "468/468 [==============================] - 25s 54ms/step - loss: 0.2131 - acc: 0.9368 - val_loss: 0.0597 - val_acc: 0.9820\n",
      "Epoch 35/200\n",
      "468/468 [==============================] - 24s 50ms/step - loss: 0.2123 - acc: 0.9367 - val_loss: 0.0614 - val_acc: 0.9821\n",
      "Epoch 36/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.2094 - acc: 0.9363 - val_loss: 0.0705 - val_acc: 0.9797\n",
      "Epoch 37/200\n",
      "468/468 [==============================] - 26s 56ms/step - loss: 0.2043 - acc: 0.9392 - val_loss: 0.0762 - val_acc: 0.9774\n",
      "Epoch 38/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.2078 - acc: 0.9381 - val_loss: 0.0677 - val_acc: 0.9794\n",
      "Epoch 39/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.2089 - acc: 0.9382 - val_loss: 0.1113 - val_acc: 0.9698\n",
      "Epoch 40/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.2014 - acc: 0.9391 - val_loss: 0.0663 - val_acc: 0.9812\n",
      "Epoch 41/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1997 - acc: 0.9395 - val_loss: 0.0761 - val_acc: 0.9781\n",
      "Epoch 42/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1996 - acc: 0.9416 - val_loss: 0.0618 - val_acc: 0.9809\n",
      "Epoch 43/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1955 - acc: 0.9415 - val_loss: 0.0907 - val_acc: 0.9760\n",
      "Epoch 44/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.2044 - acc: 0.9399 - val_loss: 0.0752 - val_acc: 0.9786\n",
      "Epoch 45/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1960 - acc: 0.9411 - val_loss: 0.0783 - val_acc: 0.9789\n",
      "Epoch 46/200\n",
      "468/468 [==============================] - 18s 38ms/step - loss: 0.1990 - acc: 0.9404 - val_loss: 0.0828 - val_acc: 0.9766\n",
      "Epoch 47/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.2004 - acc: 0.9395 - val_loss: 0.0761 - val_acc: 0.9783\n",
      "Epoch 48/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1926 - acc: 0.9425 - val_loss: 0.0828 - val_acc: 0.9770\n",
      "Epoch 49/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1969 - acc: 0.9418 - val_loss: 0.0629 - val_acc: 0.9829\n",
      "Epoch 50/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1978 - acc: 0.9418 - val_loss: 0.0711 - val_acc: 0.9810\n",
      "Epoch 51/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.1924 - acc: 0.9430 - val_loss: 0.0585 - val_acc: 0.9835\n",
      "Epoch 52/200\n",
      "468/468 [==============================] - 25s 53ms/step - loss: 0.1905 - acc: 0.9425 - val_loss: 0.0745 - val_acc: 0.9803\n",
      "Epoch 53/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1926 - acc: 0.9430 - val_loss: 0.0601 - val_acc: 0.9833\n",
      "Epoch 54/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1914 - acc: 0.9431 - val_loss: 0.0692 - val_acc: 0.9821\n",
      "Epoch 55/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1941 - acc: 0.9428 - val_loss: 0.0809 - val_acc: 0.9784\n",
      "Epoch 56/200\n",
      "468/468 [==============================] - 18s 38ms/step - loss: 0.1895 - acc: 0.9439 - val_loss: 0.0674 - val_acc: 0.9823\n",
      "Epoch 57/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1864 - acc: 0.9443 - val_loss: 0.0756 - val_acc: 0.9811\n",
      "Epoch 58/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.1870 - acc: 0.9451 - val_loss: 0.0887 - val_acc: 0.9770\n",
      "Epoch 59/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1873 - acc: 0.9448 - val_loss: 0.0854 - val_acc: 0.9772\n",
      "Epoch 60/200\n",
      "468/468 [==============================] - 19s 42ms/step - loss: 0.1866 - acc: 0.9445 - val_loss: 0.0877 - val_acc: 0.9758\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1876 - acc: 0.9445 - val_loss: 0.0834 - val_acc: 0.9753\n",
      "Epoch 62/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1877 - acc: 0.9446 - val_loss: 0.0711 - val_acc: 0.9805\n",
      "Epoch 63/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1859 - acc: 0.9448 - val_loss: 0.0690 - val_acc: 0.9812\n",
      "Epoch 64/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1880 - acc: 0.9443 - val_loss: 0.0604 - val_acc: 0.9828\n",
      "Epoch 65/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1884 - acc: 0.9448 - val_loss: 0.0612 - val_acc: 0.9831\n",
      "Epoch 66/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1871 - acc: 0.9442 - val_loss: 0.0716 - val_acc: 0.9799\n",
      "Epoch 67/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1838 - acc: 0.9472 - val_loss: 0.0797 - val_acc: 0.9794\n",
      "Epoch 68/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1854 - acc: 0.9454 - val_loss: 0.0686 - val_acc: 0.9804\n",
      "Epoch 69/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1796 - acc: 0.9463 - val_loss: 0.0681 - val_acc: 0.9815\n",
      "Epoch 70/200\n",
      "468/468 [==============================] - 21s 46ms/step - loss: 0.1816 - acc: 0.9467 - val_loss: 0.0568 - val_acc: 0.9843\n",
      "Epoch 71/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1862 - acc: 0.9454 - val_loss: 0.0616 - val_acc: 0.9824\n",
      "Epoch 72/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1808 - acc: 0.9475 - val_loss: 0.0758 - val_acc: 0.9804\n",
      "Epoch 73/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1826 - acc: 0.9464 - val_loss: 0.0472 - val_acc: 0.9871\n",
      "Epoch 74/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1808 - acc: 0.9472 - val_loss: 0.0579 - val_acc: 0.9851\n",
      "Epoch 75/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1762 - acc: 0.9465 - val_loss: 0.0654 - val_acc: 0.9837\n",
      "Epoch 76/200\n",
      "468/468 [==============================] - 18s 38ms/step - loss: 0.1820 - acc: 0.9464 - val_loss: 0.0552 - val_acc: 0.9851\n",
      "Epoch 77/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1785 - acc: 0.9463 - val_loss: 0.0649 - val_acc: 0.9811\n",
      "Epoch 78/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1830 - acc: 0.9473 - val_loss: 0.0569 - val_acc: 0.9847\n",
      "Epoch 79/200\n",
      "468/468 [==============================] - 18s 38ms/step - loss: 0.1800 - acc: 0.9475 - val_loss: 0.0675 - val_acc: 0.9827\n",
      "Epoch 80/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1781 - acc: 0.9475 - val_loss: 0.0745 - val_acc: 0.9801\n",
      "Epoch 81/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1815 - acc: 0.9461 - val_loss: 0.0731 - val_acc: 0.9813\n",
      "Epoch 82/200\n",
      "468/468 [==============================] - 21s 46ms/step - loss: 0.1770 - acc: 0.9477 - val_loss: 0.0796 - val_acc: 0.9792\n",
      "Epoch 83/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1815 - acc: 0.9469 - val_loss: 0.0516 - val_acc: 0.9859\n",
      "Epoch 84/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1834 - acc: 0.9472 - val_loss: 0.0659 - val_acc: 0.9834\n",
      "Epoch 85/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.1866 - acc: 0.9452 - val_loss: 0.0575 - val_acc: 0.9849\n",
      "Epoch 86/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1766 - acc: 0.9477 - val_loss: 0.0504 - val_acc: 0.9858\n",
      "Epoch 87/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1801 - acc: 0.9471 - val_loss: 0.0698 - val_acc: 0.9800\n",
      "Epoch 88/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1788 - acc: 0.9474 - val_loss: 0.0542 - val_acc: 0.9859\n",
      "Epoch 89/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1732 - acc: 0.9494 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 90/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1758 - acc: 0.9482 - val_loss: 0.0702 - val_acc: 0.9821\n",
      "Epoch 91/200\n",
      "468/468 [==============================] - 18s 38ms/step - loss: 0.1742 - acc: 0.9488 - val_loss: 0.0790 - val_acc: 0.9803\n",
      "Epoch 92/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1736 - acc: 0.9490 - val_loss: 0.0550 - val_acc: 0.9845\n",
      "Epoch 93/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.1747 - acc: 0.9481 - val_loss: 0.0609 - val_acc: 0.9830\n",
      "Epoch 94/200\n",
      "468/468 [==============================] - 17s 37ms/step - loss: 0.1734 - acc: 0.9501 - val_loss: 0.0646 - val_acc: 0.9839\n",
      "Epoch 95/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1740 - acc: 0.9484 - val_loss: 0.0569 - val_acc: 0.9846\n",
      "Epoch 96/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1737 - acc: 0.9492 - val_loss: 0.0585 - val_acc: 0.9843\n",
      "Epoch 97/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1776 - acc: 0.9482 - val_loss: 0.0612 - val_acc: 0.9842\n",
      "Epoch 98/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1729 - acc: 0.9491 - val_loss: 0.0637 - val_acc: 0.9844\n",
      "Epoch 99/200\n",
      "468/468 [==============================] - 25s 54ms/step - loss: 0.1700 - acc: 0.9494 - val_loss: 0.0513 - val_acc: 0.9868\n",
      "Epoch 100/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1705 - acc: 0.9495 - val_loss: 0.0818 - val_acc: 0.9790\n",
      "Epoch 101/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1712 - acc: 0.9497 - val_loss: 0.0622 - val_acc: 0.9844\n",
      "Epoch 102/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1676 - acc: 0.9508 - val_loss: 0.0649 - val_acc: 0.9817\n",
      "Epoch 103/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1766 - acc: 0.9488 - val_loss: 0.0564 - val_acc: 0.9848\n",
      "Epoch 104/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.1754 - acc: 0.9500 - val_loss: 0.0584 - val_acc: 0.9852\n",
      "Epoch 105/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1721 - acc: 0.9500 - val_loss: 0.0610 - val_acc: 0.9839\n",
      "Epoch 106/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1704 - acc: 0.9485 - val_loss: 0.0620 - val_acc: 0.9842\n",
      "Epoch 107/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1739 - acc: 0.9503 - val_loss: 0.0517 - val_acc: 0.9873\n",
      "Epoch 108/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1679 - acc: 0.9511 - val_loss: 0.0590 - val_acc: 0.9859\n",
      "Epoch 109/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1736 - acc: 0.9493 - val_loss: 0.0519 - val_acc: 0.9858\n",
      "Epoch 110/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.1686 - acc: 0.9505 - val_loss: 0.0543 - val_acc: 0.9851\n",
      "Epoch 111/200\n",
      "468/468 [==============================] - 21s 46ms/step - loss: 0.1712 - acc: 0.9497 - val_loss: 0.0683 - val_acc: 0.9821\n",
      "Epoch 112/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1761 - acc: 0.9483 - val_loss: 0.0757 - val_acc: 0.9795\n",
      "Epoch 113/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1657 - acc: 0.9514 - val_loss: 0.0503 - val_acc: 0.9867\n",
      "Epoch 114/200\n",
      "468/468 [==============================] - 20s 44ms/step - loss: 0.1706 - acc: 0.9504 - val_loss: 0.0526 - val_acc: 0.9859\n",
      "Epoch 115/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1687 - acc: 0.9502 - val_loss: 0.0715 - val_acc: 0.9826\n",
      "Epoch 116/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1726 - acc: 0.9510 - val_loss: 0.0460 - val_acc: 0.9875\n",
      "Epoch 117/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1699 - acc: 0.9510 - val_loss: 0.0519 - val_acc: 0.9862\n",
      "Epoch 118/200\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.1667 - acc: 0.9501 - val_loss: 0.0604 - val_acc: 0.9840\n",
      "Epoch 119/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1741 - acc: 0.9502 - val_loss: 0.0697 - val_acc: 0.9825\n",
      "Epoch 120/200\n",
      "468/468 [==============================] - 19s 42ms/step - loss: 0.1693 - acc: 0.9515 - val_loss: 0.0471 - val_acc: 0.9872\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1646 - acc: 0.9521 - val_loss: 0.0532 - val_acc: 0.9868\n",
      "Epoch 122/200\n",
      "468/468 [==============================] - 24s 52ms/step - loss: 0.1710 - acc: 0.9497 - val_loss: 0.0640 - val_acc: 0.9835\n",
      "Epoch 123/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1696 - acc: 0.9511 - val_loss: 0.0511 - val_acc: 0.9867\n",
      "Epoch 124/200\n",
      "468/468 [==============================] - 18s 40ms/step - loss: 0.1681 - acc: 0.9516 - val_loss: 0.0529 - val_acc: 0.9868\n",
      "Epoch 125/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1664 - acc: 0.9511 - val_loss: 0.0589 - val_acc: 0.9842\n",
      "Epoch 126/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1691 - acc: 0.9509 - val_loss: 0.0565 - val_acc: 0.9857\n",
      "Epoch 127/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1686 - acc: 0.9516 - val_loss: 0.0662 - val_acc: 0.9831\n",
      "Epoch 128/200\n",
      "468/468 [==============================] - 21s 46ms/step - loss: 0.1644 - acc: 0.9522 - val_loss: 0.0435 - val_acc: 0.9885\n",
      "Epoch 129/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1651 - acc: 0.9522 - val_loss: 0.0538 - val_acc: 0.9865\n",
      "Epoch 130/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.1672 - acc: 0.9517 - val_loss: 0.0487 - val_acc: 0.9876\n",
      "Epoch 131/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1634 - acc: 0.9524 - val_loss: 0.0726 - val_acc: 0.9825\n",
      "Epoch 132/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1685 - acc: 0.9510 - val_loss: 0.0545 - val_acc: 0.9860\n",
      "Epoch 133/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.1686 - acc: 0.9516 - val_loss: 0.0515 - val_acc: 0.9876\n",
      "Epoch 134/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1693 - acc: 0.9520 - val_loss: 0.0504 - val_acc: 0.9880\n",
      "Epoch 135/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.1649 - acc: 0.9520 - val_loss: 0.0494 - val_acc: 0.9879\n",
      "Epoch 136/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1667 - acc: 0.9504 - val_loss: 0.0543 - val_acc: 0.9854\n",
      "Epoch 137/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1636 - acc: 0.9518 - val_loss: 0.0459 - val_acc: 0.9882\n",
      "Epoch 138/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1680 - acc: 0.9512 - val_loss: 0.0609 - val_acc: 0.9845\n",
      "Epoch 139/200\n",
      "468/468 [==============================] - 24s 50ms/step - loss: 0.1623 - acc: 0.9536 - val_loss: 0.0401 - val_acc: 0.9888\n",
      "Epoch 140/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1635 - acc: 0.9519 - val_loss: 0.0624 - val_acc: 0.9840\n",
      "Epoch 141/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1614 - acc: 0.9526 - val_loss: 0.0474 - val_acc: 0.9881\n",
      "Epoch 142/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1606 - acc: 0.9525 - val_loss: 0.0583 - val_acc: 0.9857\n",
      "Epoch 143/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1704 - acc: 0.9503 - val_loss: 0.0443 - val_acc: 0.9885\n",
      "Epoch 144/200\n",
      "468/468 [==============================] - 25s 53ms/step - loss: 0.1676 - acc: 0.9522 - val_loss: 0.0577 - val_acc: 0.9865\n",
      "Epoch 145/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1606 - acc: 0.9536 - val_loss: 0.0455 - val_acc: 0.9886\n",
      "Epoch 146/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1669 - acc: 0.9511 - val_loss: 0.0427 - val_acc: 0.9875\n",
      "Epoch 147/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1625 - acc: 0.9527 - val_loss: 0.0688 - val_acc: 0.9828\n",
      "Epoch 148/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1638 - acc: 0.9531 - val_loss: 0.0734 - val_acc: 0.9811\n",
      "Epoch 149/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1641 - acc: 0.9535 - val_loss: 0.0487 - val_acc: 0.9874\n",
      "Epoch 150/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1613 - acc: 0.9538 - val_loss: 0.0585 - val_acc: 0.9857\n",
      "Epoch 151/200\n",
      "468/468 [==============================] - 24s 51ms/step - loss: 0.1654 - acc: 0.9528 - val_loss: 0.0564 - val_acc: 0.9864\n",
      "Epoch 152/200\n",
      "468/468 [==============================] - 20s 44ms/step - loss: 0.1607 - acc: 0.9529 - val_loss: 0.0449 - val_acc: 0.9883\n",
      "Epoch 153/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1592 - acc: 0.9534 - val_loss: 0.0599 - val_acc: 0.9858\n",
      "Epoch 154/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1610 - acc: 0.9538 - val_loss: 0.0491 - val_acc: 0.9881\n",
      "Epoch 155/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.1623 - acc: 0.9536 - val_loss: 0.0550 - val_acc: 0.9864\n",
      "Epoch 156/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1630 - acc: 0.9528 - val_loss: 0.0474 - val_acc: 0.9877\n",
      "Epoch 157/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1657 - acc: 0.9542 - val_loss: 0.0568 - val_acc: 0.9858\n",
      "Epoch 158/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1630 - acc: 0.9524 - val_loss: 0.0629 - val_acc: 0.9847\n",
      "Epoch 159/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1632 - acc: 0.9523 - val_loss: 0.0515 - val_acc: 0.9859\n",
      "Epoch 160/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1598 - acc: 0.9539 - val_loss: 0.0471 - val_acc: 0.9872\n",
      "Epoch 161/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1629 - acc: 0.9526 - val_loss: 0.0540 - val_acc: 0.9865\n",
      "Epoch 162/200\n",
      "468/468 [==============================] - 23s 49ms/step - loss: 0.1585 - acc: 0.9541 - val_loss: 0.0523 - val_acc: 0.9862\n",
      "Epoch 163/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1628 - acc: 0.9531 - val_loss: 0.0637 - val_acc: 0.9845\n",
      "Epoch 164/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1618 - acc: 0.9535 - val_loss: 0.0606 - val_acc: 0.9854\n",
      "Epoch 165/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1632 - acc: 0.9525 - val_loss: 0.0446 - val_acc: 0.9886\n",
      "Epoch 166/200\n",
      "468/468 [==============================] - 23s 50ms/step - loss: 0.1601 - acc: 0.9542 - val_loss: 0.0465 - val_acc: 0.9882\n",
      "Epoch 167/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1653 - acc: 0.9523 - val_loss: 0.0514 - val_acc: 0.9872\n",
      "Epoch 168/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1622 - acc: 0.9543 - val_loss: 0.0549 - val_acc: 0.9871\n",
      "Epoch 169/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1665 - acc: 0.9535 - val_loss: 0.0545 - val_acc: 0.9854\n",
      "Epoch 170/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1615 - acc: 0.9542 - val_loss: 0.0479 - val_acc: 0.9872\n",
      "Epoch 171/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1611 - acc: 0.9531 - val_loss: 0.0520 - val_acc: 0.9865\n",
      "Epoch 172/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1615 - acc: 0.9536 - val_loss: 0.0496 - val_acc: 0.9883\n",
      "Epoch 173/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1616 - acc: 0.9526 - val_loss: 0.0542 - val_acc: 0.9870\n",
      "Epoch 174/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1634 - acc: 0.9540 - val_loss: 0.0518 - val_acc: 0.9870\n",
      "Epoch 175/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1580 - acc: 0.9536 - val_loss: 0.0426 - val_acc: 0.9886\n",
      "Epoch 176/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1615 - acc: 0.9538 - val_loss: 0.0662 - val_acc: 0.9831\n",
      "Epoch 177/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1641 - acc: 0.9522 - val_loss: 0.0540 - val_acc: 0.9863\n",
      "Epoch 178/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1583 - acc: 0.9542 - val_loss: 0.0502 - val_acc: 0.9867\n",
      "Epoch 179/200\n",
      "468/468 [==============================] - 20s 44ms/step - loss: 0.1594 - acc: 0.9549 - val_loss: 0.0545 - val_acc: 0.9866\n",
      "Epoch 180/200\n",
      "468/468 [==============================] - 22s 48ms/step - loss: 0.1605 - acc: 0.9527 - val_loss: 0.0666 - val_acc: 0.9847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1561 - acc: 0.9543 - val_loss: 0.0628 - val_acc: 0.9856\n",
      "Epoch 182/200\n",
      "468/468 [==============================] - 21s 44ms/step - loss: 0.1595 - acc: 0.9537 - val_loss: 0.0530 - val_acc: 0.9878\n",
      "Epoch 183/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1634 - acc: 0.9533 - val_loss: 0.0505 - val_acc: 0.9878\n",
      "Epoch 184/200\n",
      "468/468 [==============================] - 19s 40ms/step - loss: 0.1611 - acc: 0.9542 - val_loss: 0.0455 - val_acc: 0.9874\n",
      "Epoch 185/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1628 - acc: 0.9526 - val_loss: 0.0530 - val_acc: 0.9866\n",
      "Epoch 186/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1562 - acc: 0.9539 - val_loss: 0.0420 - val_acc: 0.9874\n",
      "Epoch 187/200\n",
      "468/468 [==============================] - 21s 46ms/step - loss: 0.1606 - acc: 0.9537 - val_loss: 0.0430 - val_acc: 0.9890\n",
      "Epoch 188/200\n",
      "468/468 [==============================] - 23s 48ms/step - loss: 0.1612 - acc: 0.9545 - val_loss: 0.0550 - val_acc: 0.9861\n",
      "Epoch 189/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1643 - acc: 0.9528 - val_loss: 0.0470 - val_acc: 0.9880\n",
      "Epoch 190/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1593 - acc: 0.9551 - val_loss: 0.0484 - val_acc: 0.9880\n",
      "Epoch 191/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1595 - acc: 0.9548 - val_loss: 0.0478 - val_acc: 0.9878\n",
      "Epoch 192/200\n",
      "468/468 [==============================] - 20s 43ms/step - loss: 0.1620 - acc: 0.9534 - val_loss: 0.0504 - val_acc: 0.9875\n",
      "Epoch 193/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1640 - acc: 0.9537 - val_loss: 0.0513 - val_acc: 0.9865\n",
      "Epoch 194/200\n",
      "468/468 [==============================] - 22s 46ms/step - loss: 0.1556 - acc: 0.9553 - val_loss: 0.0537 - val_acc: 0.9860\n",
      "Epoch 195/200\n",
      "468/468 [==============================] - 19s 41ms/step - loss: 0.1567 - acc: 0.9556 - val_loss: 0.0522 - val_acc: 0.9855\n",
      "Epoch 196/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1632 - acc: 0.9532 - val_loss: 0.0533 - val_acc: 0.9867\n",
      "Epoch 197/200\n",
      "468/468 [==============================] - 22s 47ms/step - loss: 0.1578 - acc: 0.9548 - val_loss: 0.0437 - val_acc: 0.9883\n",
      "Epoch 198/200\n",
      "468/468 [==============================] - 21s 45ms/step - loss: 0.1553 - acc: 0.9551 - val_loss: 0.0542 - val_acc: 0.9848\n",
      "Epoch 199/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1545 - acc: 0.9557 - val_loss: 0.0468 - val_acc: 0.9873\n",
      "Epoch 200/200\n",
      "468/468 [==============================] - 20s 42ms/step - loss: 0.1589 - acc: 0.9542 - val_loss: 0.0461 - val_acc: 0.9886\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NumpyArrayIterator' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa80d2e93f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m          )\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                                    steps=steps)\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1769\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NumpyArrayIterator' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
    "x_test_norm = x_test/255.\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x_train, y_train_onehot, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "    x_test, y_test_onehot, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='selu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='selu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='selu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=nb_train_samples // batch_size,\n",
    "                    initial_epoch = 0\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04680367260873318\n",
      "Test accuracy: 0.9885\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate_generator(validation_generator)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
